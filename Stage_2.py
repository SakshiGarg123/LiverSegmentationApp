# -*- coding: utf-8 -*-
"""BTP stage - 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wOkrvcBzVZcUp93xJoHDjEZ08SzjTNUN
"""

import keras
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Flatten
from keras.layers import Dense
from keras.layers import Activation
from keras.layers import Dropout
from keras.layers import Input
from keras.callbacks import ModelCheckpoint
import numpy as np
import random
import cv2
import logging
import matplotlib.pyplot as plt
from keras.optimizers import Adam, SGD
from random import shuffle
import pandas as pd
from PIL import Image
from sklearn.model_selection import train_test_split
from keras import applications

from google.colab import drive

drive.mount('/content/gdrive')

img_width, img_height = 256, 256

top_model_weights_path = 'gdrive/My Drive/bottleneck_fc_model.h5'
epochs = 50
batch_size = 1


def x_generator(files, batch_size=32, sz=(256, 256)):
    index = 0
    while True:

        # extract a random batch
        batch = files['0'][index:index + batch_size]
        index = (index + batch_size) % len(files)

        # variables for collecting batches of inputs and outputs
        batch_x = []

        for f in batch:
            raw = Image.open(f)
            raw = raw.resize(sz)
            raw = np.array(raw)
            raw = raw / 255.

            # check the number of channels because some of the images are RGBA or GRAY
            if len(raw.shape) == 2:
                raw = np.stack((raw,) * 3, axis=-1)

            batch_x.append(raw)

        # preprocess a batch of images
        batch_x = np.array(batch_x)

        yield batch_x


def save_bottleneck_features():
    model = applications.VGG16(include_top=False, weights='imagenet')
    generator = x_generator(train_files, batch_size=batch_size)
    bottleneck_features_train = model.predict_generator(generator, len(train_files) // batch_size, verbose=1)
    np.save(open('gdrive/My Drive/bottleneck_features_train.npy', 'wb'), bottleneck_features_train)
    generator = x_generator(test_files, batch_size=batch_size)
    bottleneck_features_validation = model.predict_generator(generator, len(test_files) // batch_size, verbose=1)
    np.save(open('gdrive/My Drive/bottleneck_features_validation.npy', 'wb'), bottleneck_features_validation)


def train_top_model():
    train_data = np.load(open('gdrive/My Drive/bottleneck_features_train.npy', 'rb'))
    train_labels = np.array(train_files['1'])

    validation_data = np.load(open('gdrive/My Drive/bottleneck_features_validation.npy', 'rb'))
    validation_labels = np.array(test_files['1'])

    model = Sequential()
    model.add(Flatten(input_shape=train_data.shape[1:]))
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid'))

    model.compile(optimizer=Adam(lr=3e-5), loss='binary_crossentropy', metrics=['accuracy'])
    model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size,
              validation_data=(validation_data, validation_labels))
    model.save_weights(top_model_weights_path)


def image_generator(files, batch_size=32, sz=(256, 256)):
    index = 0
    while True:

        # extract a random batch
        batch = files['0'][index:index + batch_size]
        index = (index + batch_size) % len(files)

        # variables for collecting batches of inputs and outputs
        batch_x = []
        batch_y = []

        show = True
        for f in batch:

            flag = files.loc[files['0'] == f, '1'].values[0]

            batch_y.append(flag)

            # preprocess the raw images
            raw = Image.open(f)
            raw = raw.resize(sz)
            raw = np.array(raw)
            raw = raw / 255.

            # check the number of channels because some of the images are RGBA or GRAY
            if len(raw.shape) == 2:
                raw = np.stack((raw,) * 3, axis=-1)

            batch_x.append(raw)

        # preprocess a batch of images and masks
        batch_x = np.array(batch_x)
        batch_y = np.array(batch_y)

        yield (batch_x, batch_y)


df = pd.read_csv('gdrive/My Drive/Lesion_Detection.csv').drop(['Unnamed: 0'], axis=1)
batch_size = 1

train_files, test_files = train_test_split(df, test_size=0.2)

train_files.to_csv('train_files.csv')
test_files.to_csv('test_files.csv')

train_generator = image_generator(train_files, batch_size=batch_size)
test_generator = image_generator(test_files, batch_size=batch_size)

save_bottleneck_features()

train_files.to_csv('gdrive/My Drive/train_files.csv')
test_files.to_csv('gdrive/My Drive/test_files.csv')

train_top_model()

input_tensor = Input(shape=(256, 256, 3))
model = applications.VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)
print('Model loaded')
top_model = Sequential()

top_model.add(Flatten(input_shape=model.output_shape[1:]))
top_model.add(Dense(256, activation='relu'))
top_model.add(Dropout(0.5))
top_model.add(Dense(1, activation='sigmoid'))

top_model.load_weights(top_model_weights_path)

new_model = Sequential()
new_model.add(model)
new_model.add(top_model)

for layer in new_model.layers[:25]:
    layer.trainable = False

new_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=3e-5), metrics=['accuracy'])
train_generator = image_generator(train_files, batch_size=1)
test_generator = image_generator(test_files, batch_size=1)
train_steps = len(train_files) // batch_size
test_steps = len(test_files) // batch_size
# new_model.fit_generator(train_generator, steps_per_epoch=train_steps, epochs = epochs, validation_data = test_generator, validation_steps = len(test_files)//batch_size)

new_model.fit_generator(train_generator, steps_per_epoch=train_steps, epochs=epochs, validation_data=test_generator,
                        validation_steps=len(test_files) // batch_size)

# new_model.load_weights('gdrive/My Drive/transfer_learning_weights.h5')

new_model.evaluate_generator(test_generator, steps=len(test_files) // batch_size, verbose=1)

new_model.evaluate_generator(train_generator, steps=len(train_files) // batch_size, verbose=1)

# generator = image_generator(df, batch_size = batch_size)

generator = x_generator(train_files, batch_size=batch_size)
outputs = new_model.predict_generator(generator, len(train_files) // batch_size, verbose=1)
# train_files.loc[:,'2'] = outputs
# train_files

train_files.loc[:, '2'] = outputs
train_files

train_files.to_csv('gdrive/My Drive/train_prediction.csv')

# generator = image_generator(df, batch_size = batch_size)
generator = x_generator(test_files, batch_size=batch_size)
outputs = new_model.predict_generator(generator, len(test_files) // batch_size, verbose=1)
test_files['2'] = outputs
test_files.to_csv('gdrive/My Drive/test_prediction.csv')
test_files

fn = len(test_files[(test_files['1'] == 1) & (test_files['2'] < 0.5)])
fp = len(test_files[(test_files['1'] == 0) & (test_files['2'] >= 0.5)])
tn = len(test_files[(test_files['1'] == 0) & (test_files['2'] < 0.5)])
tp = len(test_files[(test_files['1'] == 1) & (test_files['2'] >= 0.5)])
print(fn, fp, tn, tp, fn + fp + tn + tp, (tp + tn) / (tp + tn + fp + fn), tp / (tp + fp), tp / (tp + fn))

fn = len(test_files[(test_files['1'] == 1) & (test_files['2'] < 0.6)])
fp = len(test_files[(test_files['1'] == 0) & (test_files['2'] >= 0.6)])
tn = len(test_files[(test_files['1'] == 0) & (test_files['2'] < 0.6)])
tp = len(test_files[(test_files['1'] == 1) & (test_files['2'] >= 0.6)])
print(fn, fp, tn, tp, fn + fp + tn + tp, (tp + tn) / (tp + tn + fp + fn), tp / (tp + fp), tp / (tp + fn))

fn = len(test_files[(test_files['1'] == 1) & (test_files['2'] < 0.4)])
fp = len(test_files[(test_files['1'] == 0) & (test_files['2'] >= 0.4)])
tn = len(test_files[(test_files['1'] == 0) & (test_files['2'] < 0.4)])
tp = len(test_files[(test_files['1'] == 1) & (test_files['2'] >= 0.4)])

print(fn, fp, tn, tp, fn + fp + tn + tp, (tp + tn) / (tp + tn + fp + fn), tp / (tp + fp), tp / (tp + fn))

# df.to_csv('gdrive/My Drive/train_prediction.csv')

x, y = next(train_generator)

data.shape

model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=(256, 256, 1)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy', optimizer=Adam(lr=3e-5), metrics=['accuracy'])
model.summary()

x, y = next(train_generator)

print(y)
plt.axis('off')
img = x[0].squeeze()
img = np.stack((img,) * 3, axis=-1)
plt.imshow(img)


def build_callbacks():
    checkpointer = ModelCheckpoint(filepath='detection_weights.h5', verbose=0, save_best_only=True,
                                   save_weights_only=True)
    callbacks = [checkpointer, PlotLearning()]
    return callbacks


# inheritance for training process plot
class PlotLearning(keras.callbacks.Callback):
    def __init__(self):
        self.best_loss = 100

    def on_train_begin(self, logs={}):
        self.i = 0
        self.x = []
        self.losses = []
        self.val_losses = []
        self.acc = []
        self.val_acc = []
        # self.fig = plt.figure()
        self.logs = []

    def on_epoch_end(self, epoch, logs={}):
        self.logs.append(logs)
        self.x.append(self.i)
        self.losses.append(logs.get('loss'))
        self.val_losses.append(logs.get('val_loss'))
        self.acc.append(logs.get('mean_iou'))
        self.val_acc.append(logs.get('val_mean_iou'))
        self.i += 1
        #         print('i=',self.i,'loss=',logs.get('loss'),'val_loss=',logs.get('val_loss'),'mean_iou=',logs.get('mean_iou'),'val_mean_iou=',logs.get('val_mean_iou'))

        if (logs.get('loss') < self.best_loss):
            model.save('gdrive/My Drive/detection_model.h5')
            self.best_loss = logs.get('loss')
            print('better loss model saved')
        # choose a random test image and preprocess


#         path = np.random.choice(test_files)
#         raw = Image.open(f'images/{path}')
#         raw = np.array(raw.resize((256, 256)))/255.
#         raw = raw[:,:,0:3]

#         #predict the mask
#         pred = model.predict(np.expand_dims(raw, 0))

#         #mask post-processing
#         msk  = pred.squeeze()
#         msk = np.stack((msk,)*3, axis=-1)
#         msk[msk >= 0.5] = 1
#         msk[msk < 0.5] = 0

#         #show the mask and the segmented image
#         combined = np.concatenate([raw, msk, raw* msk], axis = 1)
#         plt.axis('off')
#         plt.imshow(combined)
#         plt.show()

train_steps = len(train_files) // batch_size
test_steps = len(test_files) // batch_size

model.fit_generator(train_generator, steps_per_epoch=train_steps, epochs=50, validation_data=test_generator,
                    validation_steps=test_steps, callbacks=build_callbacks())
model.save_weights('lesion_weights.h5')

# classifier = Sequential()
# classifier.add(Conv2D(16, (3,3), input_shape = (256,256,1), activation='relu'))
# classifier.add(MaxPooling2D(pool_size=(2,2)))
# classifier.add(Dropout(0.25))
# classifier.add(Flatten())
# classifier.add(Dense(units = 32, activation='relu'))
# classifier.add(Dense(units = 1, activation='sigmoid'))
# adam = Adam(lr=3e-5)
# classifier.compile(optimizer=adam, loss='binary_crossentropy',metrics=['accuracy'])
# classifier.summary()

# # classifier.load_weights('gdrive/My Drive/detection_model.h5')

# model = Sequential()
# model.add(Conv2D(32, kernel_size=(3, 3),
#                  activation='relu',
#                  input_shape=(256,256,1)))
# model.add(MaxPooling2D(pool_size=(2, 2)))
# model.add(Conv2D(64, (3, 3), activation='relu'))
# model.add(MaxPooling2D(pool_size=(2, 2)))
# model.add(Dropout(0.5))
# model.add(Flatten())
# model.add(Dense(128, activation='relu'))
# model.add(Dropout(0.5))
# model.add(Dense(1, activation='sigmoid'))
# adam = Adam(lr=3e-7)
# model.compile(optimizer=adam, loss= 'binary_crossentropy',metrics=['accuracy'])
# model.summary()

# def image_generator(files, batch_size = 32, sz = (256, 256)):

#   while True:

#     #extract a random batch
#     batch = np.random.choice(files, size = batch_size)

#     #variables for collecting batches of inputs and outputs
#     batch_x = []
#     batch_y = []

#     show = True
#     for f in batch:

#         #get the masks. Note that masks are png files
#         arr = Image.open(f.replace('vol','lesion'))
#         arr = np.array(arr)
#         flag = False  # found white pixel

#         for l in range(arr.shape[0]):
#           for j in range(arr.shape[1]):
#             if (arr[l][j] == 255):
#                 flag = True  # found white
#                 break
#           if (flag):
#             break

#         #preprocess the mask
# #         mask[mask >= 2] = 0
# #         mask[mask != 0 ] = 1


#         batch_y.append(flag)

#         #preprocess the raw images
#         raw = Image.open(f'{f}')
#         raw = raw.resize(sz)
#         raw = np.array(raw)
#         raw = np.expand_dims(raw,2)
#         raw = raw/255.

#         msk = Image.open(f.replace('vol','liver'))
#         msk = msk.resize(sz)
#         msk = np.array(msk)
#         msk = np.expand_dims(msk,2)
#         msk = msk/255.

#         #check the number of channels because some of the images are RGBA or GRAY
# #         if len(raw.shape) == 2:
# #           raw = np.stack((raw,)*3, axis=-1)

# #         else:
# #           raw = raw[:,:,0:3]

#         batch_x.append(raw*msk)

#     #preprocess a batch of images and masks
#     batch_x = np.array(batch_x)
#     batch_y = np.array(batch_y)

#     yield (batch_x, batch_y)

# batch_size = 1

# all_files = []

# for i in range(0,131):
#   all_files.append('gdrive/My Drive/EnlargedWithoutBorder_Dataset/enlarged_vol'+str(i)+'a.png')
#   all_files.append('gdrive/My Drive/EnlargedWithoutBorder_Dataset/enlarged_vol'+str(i)+'c.png')
#   all_files.append('gdrive/My Drive/EnlargedWithoutBorder_Dataset/enlarged_vol'+str(i)+'s.png')
#   all_files.append('gdrive/My Drive/FlippedEnlargedWithoutBorder_Dataset/flipped_enlarged_vol'+str(i)+'a.png')
#   all_files.append('gdrive/My Drive/FlippedEnlargedWithoutBorder_Dataset/flipped_enlarged_vol'+str(i)+'c.png')
#   all_files.append('gdrive/My Drive/FlippedEnlargedWithoutBorder_Dataset/flipped_enlarged_vol'+str(i)+'s.png')
#   for j in range(0,360,60):
#     all_files.append('gdrive/My Drive/RotatedEnlargedWithoutBorder_Dataset/rotated_'+str(j)+'_enlarged_vol'+str(i)+'a.png')
#     all_files.append('gdrive/My Drive/RotatedEnlargedWithoutBorder_Dataset/rotated_'+str(j)+'_enlarged_vol'+str(i)+'c.png')
#     all_files.append('gdrive/My Drive/RotatedEnlargedWithoutBorder_Dataset/rotated_'+str(j)+'_enlarged_vol'+str(i)+'s.png')

# shuffle(all_files)

# split = int(0.70 * len(all_files))

# #split into training and testing
# train_files = all_files[0:split]
# test_files  = all_files[split:]

# train_generator = image_generator(train_files, batch_size = batch_size)
# test_generator  = image_generator(test_files, batch_size = batch_size)

# x,y = next(train_generator)

# print(y)
# plt.axis('off')
# img = x[0].squeeze()
# img = np.stack((img,)*3,axis=-1)
# plt.imshow(img)

# def build_callbacks():
#         checkpointer = ModelCheckpoint(filepath='detection_weights.h5', verbose=0, save_best_only=True, save_weights_only=True)
#         callbacks = [checkpointer, PlotLearning()]
#         return callbacks

# # inheritance for training process plot
# class PlotLearning(keras.callbacks.Callback):
#     def __init__(self):
#       self.best_loss = 100

#     def on_train_begin(self, logs={}):
#         self.i = 0
#         self.x = []
#         self.losses = []
#         self.val_losses = []
#         self.acc = []
#         self.val_acc = []
#         #self.fig = plt.figure()
#         self.logs = []
#     def on_epoch_end(self, epoch, logs={}):
#         self.logs.append(logs)
#         self.x.append(self.i)
#         self.losses.append(logs.get('loss'))
#         self.val_losses.append(logs.get('val_loss'))
#         self.acc.append(logs.get('mean_iou'))
#         self.val_acc.append(logs.get('val_mean_iou'))
#         self.i += 1
#         print('i=',self.i,'loss=',logs.get('loss'),'val_loss=',logs.get('val_loss'),'mean_iou=',logs.get('mean_iou'),'val_mean_iou=',logs.get('val_mean_iou'))

#         if(logs.get('loss')<self.best_loss):
#           classifier.save('gdrive/My Drive/detection_model.h5')
#           self.best_loss = logs.get('loss')
#           print('better loss model saved')
#         #choose a random test image and preprocess
# #         path = np.random.choice(test_files)
# #         raw = Image.open(f'images/{path}')
# #         raw = np.array(raw.resize((256, 256)))/255.
# #         raw = raw[:,:,0:3]

# #         #predict the mask
# #         pred = model.predict(np.expand_dims(raw, 0))

# #         #mask post-processing
# #         msk  = pred.squeeze()
# #         msk = np.stack((msk,)*3, axis=-1)
# #         msk[msk >= 0.5] = 1
# #         msk[msk < 0.5] = 0

# #         #show the mask and the segmented image
# #         combined = np.concatenate([raw, msk, raw* msk], axis = 1)
# #         plt.axis('off')
# #         plt.imshow(combined)
# #         plt.show()

batch_size = 1
train_steps = len(train_files) // batch_size
test_steps = len(test_files) // batch_size
classifier.fit_generator(train_generator,
                         epochs=50, steps_per_epoch=train_steps, validation_data=test_generator,
                         validation_steps=test_steps, callbacks=build_callbacks())